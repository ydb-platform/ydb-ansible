storage_config_generation: 0
static_erasure: mirror-3-dc
host_configs: # the list of available host configurations in the cluster.
- drive:
  - path: /dev/disk/by-partlabel/ydb_disk_1
    type: SSD
  - path: /dev/disk/by-partlabel/ydb_disk_2
    type: SSD
  - path: /dev/disk/by-partlabel/ydb_disk_3
    type: SSD
  host_config_id: 1
hosts:
- host: zeit-1
  host_config_id: 1
  walle_location:
    body: 1
    data_center: 'zone-a'
    rack: '1'
- host: zeit-2
  host_config_id: 1
  walle_location:
    body: 2
    data_center: 'zone-b'
    rack: '2'
- host: zeit-3
  host_config_id: 1
  walle_location:
    body: 3
    data_center: 'zone-c'
    rack: '3'
domains_config:
  # There can be only one root domain in a cluster. Domain name prefixes all scheme objects names, e.g. full name of a table table1 in database db1.
  # in a cluster with domains_config.domain.name parameter set to Root would be equal to /Root/db1/table1
  domain:
  - name: Domain0
    domain_id: 1
    storage_pool_types:
    - kind: ssd
      pool_config:
        box_id: 1
        # fault tolerance mode name - none, block-4-2, or mirror-3-dc..
        # See docs for more details https://ydb.tech/en/docs/deploy/configuration/config#domains-blob
        erasure_species: mirror-3-dc
        kind: ssd
        geometry:
          realm_level_begin: 10
          realm_level_end: 20
          domain_level_begin: 10
          domain_level_end: 256
        pdisk_filter:
        - property:
          - type: SSD  # device type to match host_configs.drive.type
        vdisk_kind: Default
  state_storage:
  - ring:
      node: [1, 2, 3]
      nto_select: 3
    ssid: 1
  security_config:
    enforce_user_token_requirement: true
    monitoring_allowed_sids:
    - "root"
    - "ydb-admin1@ldap"
    - "CN=ydb-admins,OU=demo1,DC=ydb,DC=demo@ldap"
    administration_allowed_sids:
    - "root"
    - "ydb-admin1@ldap"
    - "CN=ydb-admins,OU=demo1,DC=ydb,DC=demo@ldap"
    viewer_allowed_sids:
    - "root"
    - "ydb-admin1@ldap"
    - "CN=ydb-admins,OU=demo1,DC=ydb,DC=demo@ldap"

blob_storage_config:         # configuration of static blobstorage group.
                             # YDB uses this group to store system tablets' data, like SchemeShard
  service_set:
    groups:
    - erasure_species: mirror-3-dc # fault tolerance mode name for the static group
      rings:          # in mirror-3-dc must have exactly 3 rings or availability zones
      - fail_domains:  # first record: fail domains of the static group describe where each vdisk of the static group should be located.
        - vdisk_locations:
          - node_id: zeit-1
            pdisk_category: SSD
            path: /dev/disk/by-partlabel/ydb_disk_1
        - vdisk_locations:
          - node_id: zeit-1
            pdisk_category: SSD
            path: /dev/disk/by-partlabel/ydb_disk_2
        - vdisk_locations:
          - node_id: zeit-1
            pdisk_category: SSD
            path: /dev/disk/by-partlabel/ydb_disk_3
      - fail_domains: # second ring: fail domains of the static group describe where each vdisk of the static group should be located.
        - vdisk_locations:
          - node_id: zeit-2
            pdisk_category: SSD
            path: /dev/disk/by-partlabel/ydb_disk_1
        - vdisk_locations:
          - node_id: zeit-2
            pdisk_category: SSD
            path: /dev/disk/by-partlabel/ydb_disk_2
        - vdisk_locations:
          - node_id: zeit-2
            pdisk_category: SSD
            path: /dev/disk/by-partlabel/ydb_disk_3
      - fail_domains: # third ring: fail domains of the static group describe where each vdisk of the static group should be located.
        - vdisk_locations:
          - node_id: zeit-3
            pdisk_category: SSD
            path: /dev/disk/by-partlabel/ydb_disk_1
        - vdisk_locations:
          - node_id: zeit-3
            pdisk_category: SSD
            path: /dev/disk/by-partlabel/ydb_disk_2
        - vdisk_locations:
          - node_id: zeit-3
            pdisk_category: SSD
            path: /dev/disk/by-partlabel/ydb_disk_3

channel_profile_config:
  profile:
  - channel:
    - erasure_species: mirror-3-dc
      pdisk_category: 1   # 0=ROT, 1=SSD, 2=NVME
      storage_pool_kind: ssd
    - erasure_species: mirror-3-dc
      pdisk_category: 1
      storage_pool_kind: ssd
    - erasure_species: mirror-3-dc
      pdisk_category: 1
      storage_pool_kind: ssd
    profile_id: 0

interconnect_config:
  start_tcp: true
  encryption_mode: OPTIONAL
  path_to_certificate_file: "/opt/ydb/certs/node.crt"
  path_to_private_key_file: "/opt/ydb/certs/node.key"
  path_to_ca_file: "/opt/ydb/certs/ca.crt"

grpc_config:
    cert: "/opt/ydb/certs/node.crt"
    key: "/opt/ydb/certs/node.key"
    ca: "/opt/ydb/certs/ca.crt"
    services_enabled:
    - legacy

auth_config:
  use_builtin_auth: true
  ldap_authentication:
    host: "dc2016.ydb.demo"
    port: 389
    base_dn: "OU=demo1,DC=ydb,DC=demo"
    bind_dn: "ydb-admin1@ydb.demo"
    bind_password: "P@$$w0rd+"
    search_filter: "sAMAccountName=$username"
    use_tls:
      enable: true
      ca_cert_file: "/etc/ssl/certs/ca-certificates.crt"
      cert_require: DEMAND
  ldap_authentication_domain: "ldap"
  refresh_time: "10m"

table_service_config:
  sql_version: 1

bootstrap_config:
  shared_cache_config: {memory_limit: '4294967296'}

feature_flags:
  enable_grpc_audit: false
  enable_get_node_labels: true
  enable_views: true

log_config:
  default_level: 4
  uaclient_config:
    uri: 'localhost:16400'
    log_name: main

audit_config:
  unified_agent_backend:
    format: JSON
    log_name: audit

# Actor system config is appended to the configuration file by Ansible task.
# Provided here in the commented form for reference
#actor_system_config:
#  use_auto_config: true
#  node_type: STORAGE # STORAGE or COMPUTE
#  cpu_count: 6
